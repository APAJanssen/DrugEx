{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrugEx API\n",
    "\n",
    "An example DrugEx workflow showcasing some basic DrugEx API features. The API provides interface definitions to handle data operations and training of models needed for obtaining a molecule designer. The interface should ensure that the current code base is extensible and loosely coupled to make interoperability with different data sources seamless and to also aid in monitoring of the training processes involved.\n",
    "\n",
    "Let's import and explain some of the important API features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main package\n",
    "import drugex\n",
    "\n",
    "# important classes for data access\n",
    "from drugex.api.environ.data import ChEMBLCSV\n",
    "from drugex.api.corpus import CorpusCSV, BasicCorpus, CorpusChEMBL\n",
    "\n",
    "# important classes for QSAR modelling\n",
    "# and (de)serialization of QSAR models\n",
    "from drugex.api.environ.models import RF\n",
    "from drugex.api.environ.serialization import FileEnvSerializer, FileEnvDeserializer\n",
    "\n",
    "# classes that handle training of the exploration\n",
    "# and exploitation networks and also handle monitoring \n",
    "# of the process \n",
    "from drugex.api.model.callbacks import BasicMonitor\n",
    "from drugex.api.pretrain.generators import BasicGenerator\n",
    "\n",
    "# ingredients needed for DrugEx agent training\n",
    "from drugex.api.agent.agents import DrugExAgent\n",
    "from drugex.api.agent.callbacks import BasicAgentMonitor\n",
    "from drugex.api.agent.policy import PG\n",
    "\n",
    "# designer API (wraps the agent after it was trained)\n",
    "from drugex.api.designer.designers import BasicDesigner, CSVConsumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define some global settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (3, 5)\n",
      "1 (2, 1)\n",
      "2 (3, 5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for device in range(torch.cuda.device_count()):\n",
    "    print(device, torch.cuda.get_device_capability(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # choose a GPU device based on the info above\n",
    "    # (the higher the capability, the better)\n",
    "    torch.cuda.set_device(2)\n",
    "\n",
    "DATA_DIR=\"data\" # folder with input data files\n",
    "OUT_DIR=\"output/workflow\" # folder to store the output of this workflow\n",
    "os.makedirs(OUT_DIR, exist_ok=True) # create the output folder\n",
    "\n",
    "# define a set of gene IDs that are interesting for \n",
    "# the target that we want to design molecules for\n",
    "GENE_IDS = [\"ADORA2A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aquisition\n",
    "\n",
    "It's time to aquire the data we will need for training of our models. There are three models that we need to build so we need three separate data sets:\n",
    "\n",
    "1. Data for the exploitation model based on a random sample of 1 million molecules from the ZINC set.\n",
    "2. Data for the exploration model based on ChEMBL data we downloaded for the desired target.\n",
    "3. Data for the QSAR modelling of the environment model -> this model will bias the final generator towards more active molecules throug the a policy gradient.\n",
    "\n",
    "### Exploitation Network\n",
    "\n",
    "The exploitation network will be based on a large data set of known chemical structures. The ZINC database is a great source of data for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selected sample of 1 million molecules\n",
    "# from the ZINC database.\n",
    "# We only use this file for illustration purposes.\n",
    "# In practice, the pretrained exploitation network should\n",
    "# be provided so there will be no need for this data,\n",
    "# but we are starting from square one here.\n",
    "ZINC_CSV=os.path.join(DATA_DIR, \"ZINC.txt\")\n",
    "\n",
    "# Load SMILES data into a corpus from a CSV file (we assume\n",
    "# that we have the structures saved in a csv file in DATA_DIR).\n",
    "# Corpus is a class which provides both the vocabulary and\n",
    "# training data for a generator.\n",
    "# This corupus will be used to train the exploitation network later.\n",
    "corpus_pre = CorpusCSV(\n",
    "    update_file=ZINC_CSV\n",
    "    # The input CSV file with chemical structures as SMILES.\n",
    "    # This is the only required parameter of this class.\n",
    "\n",
    "    , vocabulary=drugex.VOC_DEFAULT\n",
    "    # A vocabulary object that defines the tokens\n",
    "    # and other options used to construct and parse SMILES.\n",
    "    # VOC_DEFAULT is a reasonable \"catch all\" default.\n",
    "\n",
    "    , smiles_column=\"CANONICAL_SMILES\"\n",
    "    # Instructs the corpus object what column to look for when\n",
    "    # extracting SMILES to update the data.\n",
    "\n",
    "    , sep='\\t'\n",
    "    # The column separator used in the CSV file\n",
    ")\n",
    "\n",
    "# Next we update the corpus (if we did not do it already).\n",
    "# The updateData() method loads and tokenizes the SMILES it finds in the CSV.\n",
    "# The tokenized data and updated vocabulary are returned to us.\n",
    "corpus_out_zinc = os.path.join(OUT_DIR, \"zinc_corpus.txt\")\n",
    "vocab_out_zinc = os.path.join(OUT_DIR, \"zinc_voc.txt\")\n",
    "if not os.path.exists(corpus_out_zinc):\n",
    "    df, voc = corpus_pre.updateData(update_voc=True)\n",
    "    # We don't really use the return values here, but they are\n",
    "    # still there if we need them for logging purposes or\n",
    "    # something else. The update_voc flag tells the \n",
    "    # update method to also update the vocabulary\n",
    "    # based on the tokens found in the SMILES strings.\n",
    "\n",
    "    # We can save our corpus data if we want to reuse it later.\n",
    "    # The CorpusCSV class has a methods\n",
    "    # that we can use to save the vocabulary and tokenized data set.\n",
    "    corpus_pre.saveCorpus(corpus_out_zinc)\n",
    "    corpus_pre.saveVoc(vocab_out_zinc)\n",
    "else:\n",
    "    # If we initialized and saved\n",
    "    # the corpus before, we just overwrite the\n",
    "    # current one with the saved one\n",
    "    corpus_pre= CorpusCSV.fromFiles(corpus_out_zinc, vocab_out_zinc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Network\n",
    "\n",
    "We will also need a corpus for the exploration network. We will load it from ChEMBL using a different implementation of the Corpus interface than we saw above. When we update a CorpusChEMBL instance, it downloads the data for us automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CorpusChEMBL uses a list of gene identifiers\n",
    "# and download activity data for all tested compounds\n",
    "# related to the particular genes.\n",
    "corpus_out_chembl = os.path.join(OUT_DIR, \"chembl_corpus.txt\")\n",
    "vocab_out_chembl = os.path.join(OUT_DIR, \"chembl_voc.txt\")\n",
    "env_data_path = os.path.join(OUT_DIR, \"{0}.txt\".format(GENE_IDS[0]))\n",
    "if not os.path.exists(corpus_out_chembl):\n",
    "    corpus_ex = CorpusChEMBL(GENE_IDS, clean_raw=True)\n",
    "\n",
    "    # lets update this corpus and save the results\n",
    "    # (same procedure as above)\n",
    "    df, voc = corpus_ex.updateData(update_voc=True)\n",
    "    corpus_ex.saveCorpus(corpus_out_chembl)\n",
    "    corpus_ex.saveVoc(vocab_out_chembl)\n",
    "\n",
    "    # in addition we will also save the raw downloaded data\n",
    "    # (this is what we will also use as a basis for the environment QSAR model)\n",
    "    corpus_ex.raw_data.to_csv(env_data_path, sep=\"\\t\", index=False)\n",
    "else:\n",
    "    # If we already generated the corpus file,\n",
    "    # we can load it using the CorpusCSV class\n",
    "    corpus_ex = CorpusCSV.fromFiles(corpus_out_chembl, vocab_out_chembl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in both cases we requested to update the vocabulary according to\n",
    "tokens found in the underlying smiles for both the zinc\n",
    "and ChEMBL corpus, we now need to unify them. Vocabularies\n",
    "can be combined using the plus operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_all = corpus_pre.voc + corpus_ex.voc\n",
    "corpus_pre.voc = voc_all\n",
    "corpus_ex.voc = voc_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did not do this, the exploitation and\n",
    "exploration networks might not be compatible\n",
    "and we would run into issues during modelling.\n",
    "\n",
    "### Environment QSAR model\n",
    "\n",
    "We also need activity data to\n",
    "train the environment QSAR model which will provide the activity\n",
    "values for policy gradient.\n",
    "Luckily, we already have the file to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environ_data = ChEMBLCSV(\n",
    "    env_data_path # we got this file from ChEMBL thanks to CorpusChEMBL\n",
    "    , 6.5 # this is the activity threshold for the pChEMBL value\n",
    "    , id_col='MOLECULE_CHEMBL_ID' # column by which we group multiple results per molecule\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ChEMBLCSV class not only loads the activity data,\n",
    "but also provides access to it for the\n",
    "QSAR learning algorithms (see below).\n",
    "\n",
    "## Model Training\n",
    "\n",
    "### Exploitation Network\n",
    "\n",
    "Training the exploitation generator takes a long time (we have over a million molecules in our ZINC sample)\n",
    "so we would like to monitor\n",
    "this process. We can use the Monitor\n",
    "interface for that. The \"BasicMonitor\" just\n",
    "saves log files and model checkpoints\n",
    "in the given directory, but we could easily\n",
    "implement our own monitor that could do much more\n",
    "TODO: it would be nice to also have a method in\n",
    "the monitor that would stop the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_monitor = BasicMonitor(\n",
    "        out_dir=OUT_DIR\n",
    "        , identifier=\"pr\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if we can load the model from disk...\n",
    "identifier = 'environ_rf'\n",
    "des = FileEnvDeserializer(OUT_DIR, identifier)\n",
    "try:\n",
    "    # The deserializer automatically looks for\n",
    "    # a model in the given directory with the given identifier\n",
    "    environ_model = des.getModel()\n",
    "    print(\"Model found at:\", des.path)\n",
    "    # return model\n",
    "except FileNotFoundError:\n",
    "    print(\"Training environment model...\")\n",
    "\n",
    "    # we choose the random forest algorithm\n",
    "    environ_model = RF(train_provider=environ_data)\n",
    "    environ_model.fit()\n",
    "    # we save the model if we need it later\n",
    "    # we also choose to save the performance data\n",
    "    ser = FileEnvSerializer(OUT_DIR, identifier, include_perf=True)\n",
    "    ser.saveModel(environ_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
