{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrugEx API\n",
    "\n",
    "An example DrugEx workflow showcasing some basic DrugEx API features. The API provides interface definitions to handle data operations and training of models needed for obtaining a molecule designer. The interface should ensure that the current code base is extensible and loosely coupled to make interoperability with different data sources seamless and to also aid in monitoring of the training processes involved.\n",
    "\n",
    "Let's import and explain some of the important API features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main package\n",
    "import drugex\n",
    "\n",
    "# important classes for data access\n",
    "from drugex.api.environ.data import ChEMBLCSV\n",
    "from drugex.api.corpus import CorpusCSV, BasicCorpus, CorpusChEMBL\n",
    "\n",
    "# important classes for QSAR modelling\n",
    "# and (de)serialization of QSAR models\n",
    "from drugex.api.environ.models import RF\n",
    "from drugex.api.environ.serialization import FileEnvSerializer, FileEnvDeserializer\n",
    "\n",
    "# classes that handle training of the exploration\n",
    "# and exploitation networks and also handle monitoring \n",
    "# of the process \n",
    "from drugex.api.model.callbacks import BasicMonitor\n",
    "from drugex.api.pretrain.generators import BasicGenerator\n",
    "\n",
    "# ingredients needed for DrugEx agent training\n",
    "from drugex.api.agent.agents import DrugExAgent\n",
    "from drugex.api.agent.callbacks import BasicAgentMonitor\n",
    "from drugex.api.agent.policy import PG\n",
    "\n",
    "# designer API (wraps the agent after it was trained)\n",
    "from drugex.api.designer.designers import BasicDesigner, CSVConsumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define some global settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (3, 5)\n",
      "1 (2, 1)\n",
      "2 (3, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sichom/software/miniconda/envs/drugex/lib/python3.5/site-packages/torch/cuda/__init__.py:117: UserWarning: \n",
      "    Found GPU1 GeForce GT 610 which is of cuda capability 2.1.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for device in range(torch.cuda.device_count()):\n",
    "    print(device, torch.cuda.get_device_capability(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # choose a GPU device based on the info above\n",
    "    # (the higher the capability, the better)\n",
    "    torch.cuda.set_device(2)\n",
    "\n",
    "DATA_DIR=\"data\" # folder with input data files\n",
    "OUT_DIR=\"output/workflow\" # folder to store the output of this workflow\n",
    "os.makedirs(OUT_DIR, exist_ok=True) # create the output folder\n",
    "\n",
    "# define a set of gene IDs that are interesting for \n",
    "# the target that we want to design molecules for\n",
    "GENE_IDS = [\"ADORA2A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aquisition\n",
    "\n",
    "It's time to aquire the data we will need for training of our models. There are three models that we need to build so we need three separate data sets:\n",
    "\n",
    "1. Data for the exploitation model based on a random sample of 1 million molecules from the ZINC set.\n",
    "2. Data for the exploration model based on ChEMBL data we downloaded for the desired target.\n",
    "3. Data for the QSAR modelling of the environment model -> this model will bias the final generator towards more active molecules throug the a policy gradient.\n",
    "\n",
    "### Exploitation Network\n",
    "\n",
    "The exploitation network will be based on a large data set of known chemical structures. The ZINC database is a great source of data for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading SMILES: 100%|██████████| 1018452/1018452 [00:03<00:00, 270090.17it/s]\n",
      "Collecting tokens:  35%|███▍      | 356337/1018451 [07:23<14:16, 773.42it/s] "
     ]
    }
   ],
   "source": [
    "# Randomly selected sample of 1 million molecules\n",
    "# from the ZINC database.\n",
    "# We only use this file for illustration purposes.\n",
    "# In practice, the pretrained exploitation network should\n",
    "# be provided so there will be no need for this data,\n",
    "# but we are starting from square one here.\n",
    "ZINC_CSV=os.path.join(DATA_DIR, \"ZINC.txt\")\n",
    "\n",
    "# Load SMILES data into a corpus from a CSV file (we assume\n",
    "# that we have the structures saved in a csv file in DATA_DIR).\n",
    "# Corpus is a class which provides both the vocabulary and\n",
    "# training data for a generator.\n",
    "# This corupus will be used to train the exploitation network later.\n",
    "corpus_pre = CorpusCSV(\n",
    "    update_file=ZINC_CSV\n",
    "    # The input CSV file with chemical structures as SMILES.\n",
    "    # This is the only required parameter of this class.\n",
    "\n",
    "    , vocabulary=drugex.VOC_DEFAULT\n",
    "    # A vocabulary object that defines the tokens\n",
    "    # and other options used to construct and parse SMILES.\n",
    "    # VOC_DEFAULT is a reasonable \"catch all\" default.\n",
    "\n",
    "    , smiles_column=\"CANONICAL_SMILES\"\n",
    "    # Instructs the corpus object what column to look for when\n",
    "    # extracting SMILES to update the data.\n",
    "\n",
    "    , sep='\\t'\n",
    "    # The column separator used in the CSV file\n",
    ")\n",
    "\n",
    "# Next we update the corpus (if we did not do it already).\n",
    "# The updateData() method loads and tokenizes the SMILES it finds in the CSV.\n",
    "# The tokenized data and updated vocabulary are returned to us.\n",
    "corpus_out_zinc = os.path.join(OUT_DIR, \"zinc_corpus.txt\")\n",
    "vocab_out_zinc = os.path.join(OUT_DIR, \"zinc_voc.txt\")\n",
    "if not os.path.exists(corpus_out_zinc):\n",
    "    df, voc = corpus_pre.updateData(update_voc=True)\n",
    "    # We don't really use the return values here, but they are\n",
    "    # still there if we need them for logging purposes or\n",
    "    # something else. The update_voc flag tells the \n",
    "    # update method to also update the vocabulary\n",
    "    # based on the tokens found in the SMILES strings.\n",
    "\n",
    "    # We can save our corpus data if we want to reuse it later.\n",
    "    # The CorpusCSV class has a methods\n",
    "    # that we can use to save the vocabulary and tokenized data set.\n",
    "    corpus_pre.saveCorpus(corpus_out_zinc)\n",
    "    corpus_pre.saveVoc(vocab_out_zinc)\n",
    "else:\n",
    "    # If we initialized and saved\n",
    "    # the corpus before, we just overwrite the\n",
    "    # current one with the saved one\n",
    "    corpus_pre= CorpusCSV.fromFiles(corpus_out_zinc, vocab_out_zinc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Network\n",
    "\n",
    "We will also need a corpus for the exploration network. We will load it from ChEMBL using a different implementation of the Corpus interface than we saw above. When we update a CorpusChEMBL instance, it downloads the data for us automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CorpusChEMBL uses a list of gene identifiers\n",
    "# and download activity data for all tested compounds\n",
    "# related to the particular genes.\n",
    "corpus_out_chembl = os.path.join(OUT_DIR, \"chembl_corpus.txt\")\n",
    "vocab_out_chembl = os.path.join(OUT_DIR, \"chembl_voc.txt\")\n",
    "env_data_path = os.path.join(OUT_DIR, \"{0}.txt\".format(GENE_IDS[0]))\n",
    "if not os.path.exists(corpus_out_chembl):\n",
    "    corpus_ex = CorpusChEMBL(GENE_IDS, clean_raw=True)\n",
    "\n",
    "    # lets update this corpus and save the results\n",
    "    # (same procedure as above)\n",
    "    df, voc = corpus_ex.updateData(update_voc=True)\n",
    "    corpus_ex.saveCorpus(corpus_out_chembl)\n",
    "    corpus_ex.saveVoc(vocab_out_chembl)\n",
    "\n",
    "    # in addition we will also save the raw downloaded data\n",
    "    # (this is what we will also use as a basis for the environment QSAR model)\n",
    "    corpus_ex.raw_data.to_csv(env_data_path, sep=\"\\t\", index=False)\n",
    "else:\n",
    "    # If we already generated the corpus file,\n",
    "    # we can load it using the CorpusCSV class\n",
    "    corpus_ex = CorpusCSV.fromFiles(corpus_out_chembl, vocab_out_chembl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in both cases we requested to update the vocabulary according to\n",
    "tokens found in the underlying smiles for both the zinc\n",
    "and ChEMBL corpus, we now need to unify them. Vocabularies\n",
    "can be combined using the plus operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_all = corpus_pre.voc + corpus_ex.voc\n",
    "corpus_pre.voc = voc_all\n",
    "corpus_ex.voc = voc_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did not do this, the exploitation and\n",
    "exploration networks might not be compatible\n",
    "and we would run into issues during modelling.\n",
    "\n",
    "### Environment QSAR model\n",
    "\n",
    "We also need activity data to\n",
    "train the environment QSAR model which will provide the activity\n",
    "values for policy gradient.\n",
    "Luckily, we already have the file to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environ_data = ChEMBLCSV(\n",
    "    env_data_path # we got this file from ChEMBL thanks to CorpusChEMBL\n",
    "    , 6.5 # this is the activity threshold for the pChEMBL value\n",
    "    , id_col='MOLECULE_CHEMBL_ID' # column by which we group multiple results per molecule\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ChEMBLCSV class not only loads the activity data,\n",
    "but also provides access to it for the\n",
    "QSAR learning algorithms (see below).\n",
    "\n",
    "## Model Training\n",
    "\n",
    "### Exploitation Network\n",
    "\n",
    "Training the exploitation generator takes a long time (we have over a million molecules in our ZINC sample)\n",
    "so we would like to monitor\n",
    "this process. We can use the Monitor\n",
    "interface for that. The \"BasicMonitor\" just\n",
    "saves log files and model checkpoints\n",
    "in the given directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_monitor = BasicMonitor(\n",
    "        out_dir=OUT_DIR\n",
    "        , identifier=\"pr\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: it would be nice to also have a method in\n",
    "the monitor that would stop the training process\n",
    "\n",
    "However, we could easily implement our own monitor that could do a bit more than just what the basic monitor does. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class MyMonitor(BasicMonitor):\n",
    "    \"\"\"\n",
    "    This monitor adds some functionality on top of the basic monitor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        This method is called after training has completed.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().close()\n",
    "        \n",
    "        # We just get the performance figure.\n",
    "        return self.getPerfFigure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_monitor = MyMonitor(\n",
    "    out_dir=OUT_DIR\n",
    "    , identifier=\"pr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monitor actually does more than just monitoring\n",
    "of the process. It also keeps track of the best\n",
    "model built yet and can be used to initialize\n",
    "a generator based on that.\n",
    "We use that feature below. If there already is\n",
    "a network state saved somewhere in our output directory, we do not do any training and just load the model from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pr_monitor.getState(): # this will be False if the monitor cannot find an existing state\n",
    "    print(\"Pretraining exploitation network...\")\n",
    "    pretrained = BasicGenerator(\n",
    "        monitor=pr_monitor\n",
    "        , corpus=corpus_pre\n",
    "        , train_params={\n",
    "            # these parameters are fed directly to the\n",
    "            # fit method of the underlying pytorch model\n",
    "            \"epochs\" : 60 # lets just make this one quick\n",
    "        }\n",
    "    )\n",
    "    pretrained.pretrain()\n",
    "    # This method also has parameters\n",
    "    # regarding partioning of the training data.\n",
    "    # We just use the defaults in this case.\n",
    "else:\n",
    "    pretrained = BasicGenerator(\n",
    "        monitor=pr_monitor\n",
    "        , initial_state=pr_monitor # the monitor provides initial state\n",
    "        , corpus=corpus_pre\n",
    "    )\n",
    "    # we will not do any training this time,\n",
    "    # but we could just continue by\n",
    "    # specifying the training parameters and\n",
    "    # calling pretrain again\n",
    "    # TODO: maybe it would be nice if the monitor\n",
    "    # keeps track of the settings as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the figure above? That is from our customized pretrainer monitor. We could configure the monitor to do much more (there are more methods than just the close method that we could override).\n",
    "\n",
    "### Exploration Network\n",
    "\n",
    "Next comes the exploration network. The approach is the same, but we use the previously trained network as the initial state. First, we define the monitor, though. We will use the one we defined above, but give it a different identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_monitor = MyMonitor(\n",
    "        out_dir=OUT_DIR\n",
    "        , identifier=\"ex\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration network fine-tunes the pretrained\n",
    "one so we have to use the pr_monitor to initialize\n",
    "the initial state of the exploartion network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ex_monitor.getState():\n",
    "    print(\"Pretraining exploration network...\")\n",
    "    exploration = BasicGenerator(\n",
    "        monitor=ex_monitor\n",
    "        , initial_state=pr_monitor # initialize from the states of the best pretrained network\n",
    "        , corpus=corpus_ex # use target-specific corpus for exploration\n",
    "        , train_params={\n",
    "            \"epochs\" : 60 # We will make this one quick too.\n",
    "        }\n",
    "    )\n",
    "    exploration.pretrain(validation_size=512)\n",
    "    # In this case we want to use a validation set.\n",
    "    # This set will be used to estimate the\n",
    "    # loss instead of the training set.\n",
    "else:\n",
    "    exploration = BasicGenerator(\n",
    "        monitor=ex_monitor\n",
    "        , initial_state=ex_monitor\n",
    "        , corpus=corpus_ex\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Model\n",
    "\n",
    "This model will provide the environment for the policy gradient. We already got the data to train this model and saved it to the `environ_data`. This is a data provider for the QSAR model and can be used with other algorithms implemented in the library. However, we will just limit ourselves to random forest in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if we can load the model already from disk \n",
    "# using the standard deserializer...\n",
    "identifier = 'environ_rf'\n",
    "des = FileEnvDeserializer(OUT_DIR, identifier)\n",
    "try:\n",
    "    # The deserializer automatically looks for\n",
    "    # a model in the given directory with the given identifier\n",
    "    environ_model = des.getModel()\n",
    "    print(\"Model found at:\", des.path)\n",
    "except FileNotFoundError:\n",
    "    # if the model is nowhere to be found, we train and save it\n",
    "    print(\"Training environment model...\")\n",
    "\n",
    "    environ_model = RF(train_provider=environ_data)\n",
    "    environ_model.fit()\n",
    "    # we save the model so that we don't have to train again next time\n",
    "    # we also choose to save the performance data (this will\n",
    "    # also save a ROC curve figure in our output directory\n",
    "    # to check performance)\n",
    "    ser = FileEnvSerializer(OUT_DIR, identifier, include_perf=True)\n",
    "    ser.saveModel(environ_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DrugEx Agent\n",
    "\n",
    "We now have all ingredients to train\n",
    "the DrugEx agent. First, we\n",
    "need to define the policy gradient\n",
    "strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PG( # So far this is the only policy there is in the API\n",
    "    batch_size=512\n",
    "    , mc=10 # number of repeated samples\n",
    "    , epsilon=0.01\n",
    "    , beta=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DrugEx agents have their own monitors.\n",
    "The basic one saves monitoring results to files as well and generally uses the same pattern as we have seen with generators to keep up to date with the best state of the model and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = 'e_%.2f_%.1f_%dx%d' % (policy.epsilon, policy.beta, policy.batch_size, policy.mc)\n",
    "agent_monitor = BasicAgentMonitor(OUT_DIR, identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the DrugEx agent itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not agent_monitor.getState():\n",
    "    print(\"Training DrugEx agent...\")\n",
    "    agent = DrugExAgent(\n",
    "        agent_monitor # our monitor\n",
    "        , environ_model # environment for the policy gradient\n",
    "        , pretrained # the pretrained model\n",
    "        , policy # our policy gradient implemntation\n",
    "        , exploration # the fine-tuned model\n",
    "        , {\n",
    "            \"n_epochs\" : 60 # quick again\n",
    "        }\n",
    "    )\n",
    "    agent.train()\n",
    "else:\n",
    "    # The DrugEx agent monitor also provides\n",
    "    # a generator state -> it is the\n",
    "    # best model from training. We can\n",
    "    # therefore create a generator \n",
    "    # based on this initial state just like we did before:\n",
    "    agent = BasicGenerator(\n",
    "        initial_state=agent_monitor\n",
    "        , corpus=BasicCorpus(\n",
    "            # If we are not training the generator,\n",
    "            # we can just provide a basic corpus\n",
    "            # that only provides vocabulary\n",
    "            # and no corpus data -> we\n",
    "            # only have to specify the right\n",
    "            # vocabulary, which is the one of\n",
    "            # the exploration or exploitation network.\n",
    "            # We choose the exploration network here:\n",
    "            vocabulary=corpus_pre.voc\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing DrugEx Designer\n",
    "\n",
    "From a fully trained DrugEx agent generator,\n",
    "we can create a designer class which\n",
    "will handle sampling of SMILES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = CSVConsumer(\n",
    "    # a CSV file containing not just SMILES, \n",
    "    # but also scores as determined by the environment model.\n",
    "    os.path.join(OUT_DIR, 'designer_mols.csv')\n",
    ")\n",
    "designer = BasicDesigner(\n",
    "    agent=agent # our agent\n",
    "    , consumer=consumer # use this consumer to return results\n",
    "    , n_samples=1000 # number of SMILES to sample in total\n",
    "    , batch_size=512 # number of SMILES to sample in one batch\n",
    ")\n",
    "designer() # design the molecules\n",
    "consumer.save() # save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
