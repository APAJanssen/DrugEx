#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""This file is used for generator training under reinforcement learning framework.

It is implemented by integrating exploration strategy into REINFORCE algorithm.
The deep learning code is implemented by PyTorch ( >= version 1.0)
"""

import os

import click
import numpy as np
import torch
from rdkit import rdBase
from torch.utils.data import DataLoader, TensorDataset
from tqdm import trange

from drugex import model, util


def policy_gradient(agent, environ, explore=None, *, batch_size, mc, epsilon, baseline):
    """Training generator under reinforcement learning framework,
    The rewoard is only the final reward given by environment (predictor).

    agent (model.Generator): the exploitation network for SMILES string generation
    environ (util.Activity): the environment provide the final reward for each SMILES
    explore (model.Generator): the exploration network for SMILES string generation,
        it has the same architecture with the agent.
    """
    seqs = []

    # repeated sampling with MC times
    for _ in range(mc):
        seq = agent.sample(batch_size, explore=explore, epsilon=epsilon)
        seqs.append(seq)
    seqs = torch.cat(seqs, dim=0)
    ix = util.unique(seqs)
    seqs = seqs[ix]
    smiles, valids = util.check_smiles(seqs, agent.voc)

    # obtaining the reward
    preds = environ(smiles)
    preds[valids == False] = 0
    preds -= baseline
    preds = torch.Tensor(preds.reshape(-1, 1)).to(util.dev)

    ds = TensorDataset(seqs, preds)
    loader = DataLoader(ds, batch_size=batch_size)

    # Training Loop
    for seq, pred in loader:
        score = agent.likelihood(seq)
        agent.optim.zero_grad()
        loss = agent.PGLoss(score, pred)
        loss.backward()
        agent.optim.step()


def rollout_pg(agent, environ, explore=None, *, batch_size, baseline, mc, epsilon):
    """Training generator under reinforcement learning framework.

    The reward is given for each token in the SMILES, which is generated by
    Monte Carlo Tree Search based on final reward given by the environment.

    Arguments:

        agent (model.Generator): the exploitation network for SMILES string generation
        environ (util.Activity): the environment provide the final reward for each SMILES
        explore (model.Generator): the exploration network for SMILES string generation,
            it has the same architecture with the agent.
    """

    agent.optim.zero_grad()
    seqs = agent.sample(batch_size, explore=explore, epsilon=epsilon)
    batch_size = seqs.size(0)
    seq_len = seqs.size(1)
    rewards = np.zeros((batch_size, seq_len))
    smiles, valids = util.check_smiles(seqs, agent.voc)
    preds = environ(smiles) - baseline
    preds[valids == False] = - baseline
    scores, hiddens = agent.likelihood(seqs)

    # Monte Carlo Tree Search for step rewards generation
    for _ in trange(mc):
        for i in range(seq_len):
            if (seqs[:, i] != 0).any():
                h = hiddens[:, :, i, :]
                subseqs = agent.sample(batch_size, inits=(seqs[:, i], h, i + 1, None))
                subseqs = torch.cat([seqs[:, :i+1], subseqs], dim=1)
                subsmile, subvalid = util.check_smiles(subseqs, voc=agent.voc)
                subpred = environ(subsmile) - baseline
                subpred[1 - subvalid] = -baseline
            else:
                subpred = preds
            rewards[:, i] += subpred
    loss = agent.PGLoss(scores, seqs, torch.FloatTensor(rewards / mc))
    loss.backward()
    agent.optim.step()
    return 0, valids.mean(), smiles, preds


def _main_helper(*, epsilon, baseline, batch_size, mc, vocabulary_path, output_dir):
    #: Vocabulary containing all of the tokens for SMILES construction
    voc = util.Voc(vocabulary_path)
    #: File path of predictor in the environment
    environ_path = os.path.join(output_dir, 'RF_cls_ecfp6.pkg')
    #: file path of hidden states in RNN for initialization
    initial_path = os.path.join(output_dir, 'net_p.pkg')
    #: file path of hidden states of optimal exploitation network
    agent_path = os.path.join(output_dir, 'net_e_%.2f_%.1f_%dx%d' % (epsilon, baseline, batch_size, mc))
    #: file path of hidden states of exploration network
    explore_path = os.path.join(output_dir, 'net_p.pkg')

    # Environment (predictor)
    environ = util.Environment(environ_path)
    # Agent (generator, exploitation network)
    agent = model.Generator(voc)
    agent.load_state_dict(torch.load(initial_path))

    # exploration network
    explore = model.Generator(voc)
    explore.load_state_dict(torch.load(explore_path))

    best_score = 0
    log_file = open(agent_path + '.log', 'w')

    it = trange(1000)
    for epoch in it:
        it.write('\n--------\nEPOCH %d\n--------' % (epoch + 1))
        it.write('\nForward Policy Gradient Training Generator : ')
        policy_gradient(agent, environ, explore=explore,
                        baseline=baseline, batch_size=batch_size, mc=mc, epsilon=epsilon)
        seqs = agent.sample(1000)
        ix = util.unique(seqs)
        smiles, valids = util.check_smiles(seqs[ix], agent.voc)
        scores = environ(smiles)
        scores[valids == False] = 0
        unique = (scores >= 0.5).sum() / 1000
        # The model with best percentage of unique desired SMILES will be persisted on the hard drive.
        if best_score < unique:
            torch.save(agent.state_dict(), agent_path + '.pkg')
            best_score = unique
        print("Epoch+: %d average: %.4f valid: %.4f unique: %.4f" % (epoch, scores.mean(), valids.mean(), unique), file=log_file)
        for i, smile in enumerate(smiles):
            print('%f\t%s' % (scores[i], smile), file=log_file)

        # Learing rate exponential decay
        for param_group in agent.optim.param_groups:
            param_group['lr'] *= (1 - 0.01)
    log_file.close()


@click.command()
@click.option('-d', '--input-directory', type=click.Path(file_okay=False, dir_okay=True), show_default=True)
@click.option('-o', '--output-directory', type=click.Path(file_okay=False, dir_okay=True), show_default=True)
@click.option('--mc', type=int, default=10, show_default=True)
@click.option('--batch-size', type=int, default=500, show_default=True)
@click.option('--num-threads', type=int, default=1, show_default=True)
@click.option('-e', '--epsilon', type=float, default=0.1, show_default=True)
@click.option('-b', '--baseline', type=float, default=0.1, show_default=True)
@click.option('-g', '--cuda-visible-devices')
def main(input_directory, output_directory, mc, batch_size, num_threads, epsilon, baseline, cuda_visible_devices):
    rdBase.DisableLog('rdApp.error')
    torch.set_num_threads(num_threads)
    if cuda_visible_devices:
        os.environ["CUDA_VISIBLE_DEVICES"] = cuda_visible_devices
    _main_helper(
        baseline=baseline,
        batch_size=batch_size,
        mc=mc,
        epsilon=epsilon,
        vocabulary_path=os.path.join(input_directory, "voc.txt"),
        output_dir=output_directory,
    )


if __name__ == "__main__":
    main()
